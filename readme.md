

# ğŸ§  Local Llama 3.1 ChatGPT Clone (React + FastAPI + Ollama)

A full-stack AI chatbot built using:

- âš› React (Frontend)
- ğŸš€ FastAPI (Backend)
- ğŸ¦™ Ollama (Llama 3.1 local LLM)
- ğŸ”„ Streaming responses
- ğŸ›‘ Stop generation feature
- âœ¨ Markdown rendering (bold, code, formatting supported)

This project replicates a ChatGPT-style interface running entirely on your local machine.

---

# ğŸ“¸ Features

- ChatGPT-style UI
- Real-time streaming responses
- Stop generation button
- Markdown rendering (bold, lists, code blocks)
- Auto-scroll to latest message
- Enter to send / Shift + Enter for new line
- Clean full-screen professional layout
- Fully local (no external API usage)

---

# âš™ï¸ Requirements

Before starting, make sure you have:

- Node.js (LTS version)
- Python 3.9+
- Git
- Ollama installed

---

# ğŸ¦™ Step 1 â€” Install Ollama

Download from:

https://ollama.com

After installation, verify:

```bash
ollama --version
````

Pull Llama 3.1 model:

```bash
ollama pull llama3.1
```

Test it:

```bash
ollama run llama3.1
```

Type `/bye` to exit.

---

# ğŸ“¥ Step 2 â€” Clone Repository

```bash
git clone https://github.com/YOUR_USERNAME/ai-chatbot.git
cd ai-chatbot
```

---

# ğŸš€ Step 3 â€” Backend Setup (FastAPI)

### 1ï¸âƒ£ Navigate to backend

```bash
cd backend
```

### 2ï¸âƒ£ Create virtual environment

```bash
python -m venv venv
```

### 3ï¸âƒ£ Activate virtual environment

**Windows:**

```bash
venv\Scripts\activate
```

**Mac/Linux:**

```bash
source venv/bin/activate
```

### 4ï¸âƒ£ Install dependencies

```bash
pip install fastapi uvicorn requests python-dotenv
```

### 5ï¸âƒ£ Run backend server

```bash
uvicorn main:app --reload
```

Backend will run at:

```
http://127.0.0.1:8000
```

Keep this terminal running.

---

# ğŸ¨ Step 4 â€” Frontend Setup (React)

Open a new terminal.

Navigate to frontend:

```bash
cd frontend
```

### 1ï¸âƒ£ Install dependencies

```bash
npm install
```

### 2ï¸âƒ£ Install Markdown renderer

```bash
npm install react-markdown
```

### 3ï¸âƒ£ Start frontend

```bash
npm run dev
```

Frontend will run at:

```
http://localhost:5173
```

Open it in your browser.

---

# ğŸ›‘ Stop Generation Feature

The Stop button works using:

* `AbortController` in frontend
* Cancels streaming fetch request
* Immediately stops model output

---

# ğŸ§  How It Works

1. User sends message from React UI
2. React sends conversation history to FastAPI
3. FastAPI forwards request to Ollama `/api/chat`
4. Ollama streams response
5. FastAPI relays stream to frontend
6. React updates UI live

---

# ğŸ§ª Development Commands Cheatsheet

## Backend

Start server:

```bash
uvicorn main:app --reload
```

Deactivate venv:

```bash
deactivate
```

---

## Frontend

Install packages:

```bash
npm install
```

Start dev server:

```bash
npm run dev
```

Build production:

```bash
npm run build
```

---

## Ollama

List models:

```bash
ollama list
```

Remove model:

```bash
ollama rm llama3.1
```

Serve manually:

```bash
ollama serve
```

---

# ğŸ›  Common Errors & Fixes

### CORS Error

Make sure backend has:

```python
allow_origins=["http://localhost:5173"]
```

---

### Backend not connecting

Ensure Ollama is running:

```bash
ollama list
```

---

### Port already in use

Kill process or change port:

```bash
uvicorn main:app --reload --port 8001
```

---

# ğŸŒŸ Future Improvements

* Chat history persistence
* Model selector dropdown
* Sidebar for conversation list
* Code syntax highlighting
* Dark/light mode toggle
* Authentication system
* Docker support
* WebSocket streaming

---

# ğŸ¤ Contributing

Contributions are welcome.

## ğŸ” How to Contribute

### 1ï¸âƒ£ Fork the repository

Click **Fork** button on GitHub.

### 2ï¸âƒ£ Clone your fork

```bash
git clone https://github.com/YOUR_USERNAME/ai-chatbot.git
cd ai-chatbot
```

### 3ï¸âƒ£ Create a new branch

```bash
git checkout -b feature/your-feature-name
```

### 4ï¸âƒ£ Make changes

Edit files and test locally.

### 5ï¸âƒ£ Commit changes

```bash
git add .
git commit -m "Added feature: description"
```

### 6ï¸âƒ£ Push to your fork

```bash
git push origin feature/your-feature-name
```

### 7ï¸âƒ£ Create Pull Request

Go to your fork on GitHub
Click **Compare & Pull Request**

---

# ğŸ“œ License

This project is open-source and available under the MIT License.

---
